---
title: "NYC Taxi in Spark"
author: "Kevin Putschko"
date: "2/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, error = FALSE)

pacman::p_load(tidyverse, sparklyr, sparklyr.nested,
               dbplot, leaflet, DT, broom, RColorBrewer,
               plotly)
```

## Introduction

This data set has been making the rounds in the blog-sphere as a demonstration of how to analyze a large dataset using various technologies.  The original article can be found here [...] and the blog that inspired me to explore this version in R and Spark can be found here [...].

The data comes from the NYC Government website [...] which offers data on taxi rides between 2009 and 2019.  Not all of this data is usable, due to changes in how the data was collected.  So we're going to limit our analysis here to the years [...]. 

## Connecting to Spark

I had limited experience with Spark prior to conducting this analysis.  Most of my knowledge comes from the book [...] found here [...].  It's a wonderful resource for getting started with `R` and `sparklyr`.  As of writing this, I'm using `sparklyr 1.1.0` with `Spark 2.3.3`.

The cluster I'm using is on [...] and I'm storing the data in [...], and the size of the full data set is [...].

```{r connect, echo = TRUE}
sc <- spark_connect(master = "local", version = "2.3")
```

## Loading Data

In the GitHub repo for this project, found here [...] you can find the `2_download_files.rmd` file that I used to get all the data from the NYC website using the Firefox extension DownThemAll.  Once we have the raw csv files downloaded, we did some minor data cleaning, and converted them to parquet files for compression and speedier imports in the future.  You can find the file [...] to see this process.

In real life, a situation like this might have a couple processes running: collecting past data and analyzing it to help inform decsions about the future, along with collecting and analyzing incoming data in real time.  Thus, the larger historical data would be analyzed in batches, while the incoming data would be analyzed as a stream of data points.  

To set up the simulation for this analysis, I'm going to split the data into `historical`, `prediction`, and `streaming` datasets.  The `historical` data for our batch analyses including clustering and predictions based on the past performance of taxi data.  The `prediction` data will be have clusters applied, and then used for predicting the number of riders per [...], the number of drivers needed for the [...], the average fare per mile, and the expected total tips for the [...].  Finally, the `streaming` data will be used for the real time analysis, with data points showing up one at a time, then we assign a cluster, run the prediction models, and monitor the number of available drivers.

```{r load}
data_raw <- 
  spark_read_parquet(
    sc, 
    name = "full_data", 
    path = "C:/Users/exp01754/OneDrive/Data/cs_bignyctaxi/data-small/parquet/yellow_tripdata_2009-01")
```

[...]

```{r prep}
# When sparklyr doesn't have an r function to interpret a spark function, use sql in quotes
# pickup_hour = sql("hour(`trip_pickup_datetime`)"))

# Date functions come from https://www.obstkel.com/spark-sql-date-functions

data_prep <- 
  data_raw %>%
  rename_all(str_to_lower) %>%
  mutate_at("payment_type", str_to_lower) %>%
  filter(start_lon > -74.05,  start_lon < -73.75,
         start_lat >  40.58,  start_lat <  40.90,
         end_lon > -74.05,    end_lon < -73.75,
         end_lat >  40.58,    end_lat <  40.90,
         passenger_count > 0, passenger_count < 7,
         trip_distance > 0,   trip_distance < 100,
         total_amt > 0,
         !is.na(trip_pickup_datetime)) %>%
  mutate(trip_pickup_datetime = to_utc_timestamp(trip_pickup_datetime, "UTC")) %>% 
  mutate(pickup_date = date_format(trip_pickup_datetime, "YYYY-MM-DD"),
         pickup_year = date_format(trip_pickup_datetime, "YYYY"),
         pickup_mon  = date_format(trip_pickup_datetime, "MM"),
         pickup_day  = date_format(trip_pickup_datetime, "DD"),
         pickup_hour = sql("hour(`trip_pickup_datetime`)"),
         pickup_wday = date_format(trip_pickup_datetime, "E"),
         pickup_nday = dayofweek(trip_pickup_datetime)) %>% 
  select(-trip_pickup_datetime) %>% 
  compute("data_prep")

reference_wday <- 
  data_prep %>% 
  distinct(pickup_wday, pickup_nday) %>% 
  arrange(pickup_nday) %>% 
  collect() %>% 
  mutate_at("pickup_wday", fct_inorder, ordered = TRUE)
```

[...]

```{r split}
data_split <- 
  data_prep %>% 
  sdf_random_split(historical = 0.40, 
                   prediction = 0.50,
                   streaming  = 0.10,
                   seed = 42)

data_historical <- data_split$historical
data_prediction <- data_split$prediction
data_streaming  <- data_split$streaming

data_counts <- 
  data_split %>% 
  map(count) %>% 
  map(collect) %>% 
  enframe(name = "dataframe") %>% 
  unnest(value)

data_counts %>% DT::datatable(rownames = FALSE)
```

## Exploration

These plots are inspired by the original blog post. [...]

```{r heatmap-data}
# save the map to file so we don't have to wait for it here

time_heatmap <- 
  system.time({
    plot_data_heatmap <- 
      data_historical %>% 
      group_by(pickup_nday, pickup_hour) %>%
      summarise(n = count(),
                fpm = sum(fare_amt, na.rm = TRUE) / sum(trip_distance, na.rm = TRUE)) %>%
      collect() %>%
      complete(pickup_nday, pickup_hour, fill = list(n = 0)) %>%
      arrange(pickup_nday, pickup_hour) %>% 
      left_join(reference_wday, by = "pickup_nday") %>% 
      mutate_at("pickup_hour", str_pad, width = 2, pad = "0", side = "left") %>% 
      mutate_at("pickup_hour", factor)
  })[[3]]

# mutate_at("pickup_wday", factor, levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")) %>%
```

Here we see [...]

This was computed in `r time_heatmap` seconds. [...]

```{r heatmap-plot}
plot_data_heatmap %>%
  ggplot(aes(x = pickup_hour, y = pickup_wday, fill = n)) +
  geom_tile() +
  scale_fill_viridis_c(label = scales::comma_format())

plot_data_heatmap %>%
  ggplot(aes(x = pickup_hour, y = pickup_wday, fill = fpm)) +
  geom_tile() +
  scale_fill_viridis_c(label = scales::dollar_format())

```

This is [...]

```{r blackmap-data}
# just make it look pretty
# save the map to file so we don't have to wait for it here

time_blackmap <- system.time({
  plot_data_blackmap <-
    data_historical %>%
    dbplot::db_compute_raster(x = start_lon, y = start_lat, resolution = 500) %>%
    rename(n = `n()`)
})[[3]]
```

We see [...]

```{r blackmap-plot}
plot_data_blackmap %>% 
  leaflet() %>% 
  addProviderTiles(providers$CartoDB.DarkMatterNoLabels) %>%
  addCircles(lng = ~start_lon, 
             lat = ~start_lat, 
             opacity = ~n, 
             radius = ~n,
             stroke = FALSE, 
             color = "yellow")
```

This was computed in `r time_blackmap` seconds.

## Historical Clusters

Now, [...]

-- analyze attributes of these 5 location hubs, 
-- w/ riders and fare/mile and average tip and %payment-type
-- leaflet map

```{r cluster}
model_cluster <- 
  data_historical %>%
  ml_kmeans(~ start_lat + start_lon, k = 5, seed = 42, prediction_col = "cluster")

data_cluster_centers <- 
  model_cluster %>% 
  broom::tidy() %>% 
  arrange(-size) %>% 
  mutate(taxi_hub = LETTERS[sequence(n())] %>% as_factor(),
         hub_color = brewer.pal(n(), "Pastel1"),
         hub_pct = size / sum(size))

data_cluster_summary <- 
  model_cluster %>% 
  ml_summary("predictions") %>% 
  group_by(cluster) %>% 
  summarise_at(c("passenger_count", "trip_distance", "fare_amt", "tip_amt"), "mean", na.rm = TRUE) %>% 
  collect()

# not going to summarise categorical columns

data_cluster_centers %>%
  left_join(data_cluster_summary, by = "cluster") %>% 
  leaflet() %>%
  addProviderTiles(providers$CartoDB.DarkMatterNoLabels) %>%
  addCircleMarkers(lng = ~ start_lon,
                   lat = ~ start_lat,
                   label = ~str_c("Hub - ", taxi_hub),
                   color = ~hub_color,
                   stroke = TRUE,
                   weight = 2,
                   opacity = .50,
                   fillColor = "transparent",
                   radius = ~if_else(hub_pct > .20, 30, 50))
```

## Predictions

Build these models once per year/quarter.

When planning for a day, run the models to predict rides, passengers, distance, fares, tips

I'm using regression models right now for the proof of concept.  They are not good models.

-- lead to expected number of drivers needed
-- Plotly Tiles with Day by Hour for Predictions

```{r predict}
data_predict_prep <- 
  ml_predict(model_cluster, data_prediction) %>% 
  group_by(cluster, pickup_nday, pickup_hour) %>% 
  summarise(rides = count(),
            passengers = sum(passenger_count, na.rm = TRUE),
            distance = sum(trip_distance, na.rm = TRUE),
            fare = sum(fare_amt, na.rm = TRUE),
            tip = sum(tip_amt, na.rm = TRUE)) %>% 
  mutate(ppr = passengers / rides,
         dpr = distance / rides,
         fpr = fare / rides,
         tpr = tip / rides,
         fpm = fare / distance,
         tpm = tip / distance) %>% 
  arrange(cluster, pickup_nday, pickup_hour) %>% 
  compute("prediction_preparation")

data_predict_encode <-
  data_predict_prep %>% 
  ft_one_hot_encoder("cluster", "en_cluster") %>% 
  ft_one_hot_encoder("pickup_nday", "en_nday")

time_model <- system.time({
  model_list <- list(
    rides = ml_linear_regression(data_predict_encode, rides ~ en_cluster + en_nday + pickup_hour),
    ppr   = ml_linear_regression(data_predict_encode,   ppr ~ en_cluster + en_nday + pickup_hour),
    dpr   = ml_linear_regression(data_predict_encode,   dpr ~ en_cluster + en_nday + pickup_hour),
    fpr   = ml_linear_regression(data_predict_encode,   fpr ~ en_cluster + en_nday + pickup_hour),
    tpr   = ml_linear_regression(data_predict_encode,   tpr ~ en_cluster + en_nday + pickup_hour),
    fpm   = ml_linear_regression(data_predict_encode,   fpm ~ en_cluster + en_nday + pickup_hour),
    tpm   = ml_linear_regression(data_predict_encode,   tpm ~ en_cluster + en_nday + pickup_hour))
})[[3]]

# data_predict_prep %>% 
#   distinct(cluster, pickup_nday, pickup_hour) %>% 
#   arrange(cluster, pickup_nday, pickup_hour)

predict_grid <- 
  list(cluster = 0:4,
       pickup_nday = 1:7,
       pickup_hour = 0:23) %>% 
  cross_df() %>% 
  left_join(reference_wday, by = "pickup_nday") %>% 
  arrange(cluster, pickup_nday, pickup_hour) %>% 
  copy_to(sc, .) %>% 
  ft_one_hot_encoder("cluster", "en_cluster") %>% 
  ft_one_hot_encoder("pickup_nday", "en_nday")

# Turn this into a function?
time_predictions <- 
  system.time({
    predict_list <- list(
      rides = predict_grid %>% 
        ml_predict(model_list$rides, .) %>% rename(p_rides = prediction) %>% collect(),
      ppr   = predict_grid %>% ml_predict(model_list$ppr, .) %>% rename(p_ppr = prediction) %>% collect(),
      dpr   = predict_grid %>% ml_predict(model_list$dpr, .) %>% rename(p_dpr = prediction) %>% collect(),
      fpr   = predict_grid %>% ml_predict(model_list$fpr, .) %>% rename(p_fpr = prediction) %>% collect(),
      tpr   = predict_grid %>% ml_predict(model_list$tpr, .) %>% rename(p_tpr = prediction) %>% collect(),
      fpm   = predict_grid %>% ml_predict(model_list$fpm, .) %>% rename(p_fpm = prediction) %>% collect(),
      tpm   = predict_grid %>% ml_predict(model_list$tpm, .) %>% rename(p_tpm = prediction) %>% collect()
    )  
  })[[3]]

predict_grid_result <- 
  predict_list %>% 
  map(select, cluster, pickup_wday, pickup_hour, last_col()) %>% 
  reduce(left_join, by = c("cluster", "pickup_wday", "pickup_hour")) %>% 
  mutate(p_rides = ifelse(p_rides < 0, 0, p_rides))

predict_grid_result %>% 
  mutate_at(vars(p_rides), round, 2) %>% 
  mutate_at(vars(p_ppr, p_dpr), scales::number, accuracy = 0.01) %>% 
  mutate_at(vars(p_fpr:p_tpm), scales::dollar, accuracy = 0.01) %>% 
  filter(cluster == 0) %>% 
  plot_ly() %>% 
  add_heatmap(x = ~pickup_hour, y = ~pickup_wday %>% fct_inorder(), z = ~p_rides, 
              text = ~str_glue("Rides: {p_rides}, People/Ride: {p_ppr}"))

```


## Live Data

-- toggle spark streaming
-- accept new start lat/lon single point
-- predict cluster
-- predict riders / fare / tip
-- monitor hub capacity

Setting up pipelines...
https://therinspark.com/pipelines.html

```{r stream}

pipeline_cluster <- 
  ml_pipeline(sc) %>% 
  ft_vector_assembler(input_cols = c("start_lat", "start_lon"), 
                      output_col = "cluster_x") %>% 
  ml_kmeans(features_col = "cluster_x",
            prediction_col = "cluster",
            seed = 42)


fx_prepare_predict <- function(x) {
  x %>% 
    group_by(cluster, pickup_nday, pickup_hour) %>% 
    summarise(rides = count(),
              passengers = sum(passenger_count, na.rm = TRUE),
              distance = sum(trip_distance, na.rm = TRUE),
              fare = sum(fare_amt, na.rm = TRUE),
              tip = sum(tip_amt, na.rm = TRUE)) %>% 
    mutate(ppr = passengers / rides,
           dpr = distance / rides,
           fpr = fare / rides,
           tpr = tip / rides,
           fpm = fare / distance,
           tpm = tip / distance) %>% 
    arrange(cluster, pickup_nday, pickup_hour)
}

pipeline_predict <- function(predict_y){
  ml_pipeline(sc) %>% 
    ft_one_hot_encoder_estimator(
      input_cols = c("pickup_nday", "cluster"), 
      output_cols = c("en_nday", "en_cluster")) %>% 
    ft_vector_assembler(input_cols = c("en_cluster", "en_nday", "pickup_hour"),
                        output_col = "predict_x") %>% 
    ml_linear_regression(features_col = "predict_x",
                         label_col = predict_y,
                         prediction_col = str_c("p_", predict_y))
}


pipe_cl <- 
  data_streaming %>% 
  sdf_sample(0.001, replacement = FALSE) %>% 
  ml_fit_and_transform(pipeline_cluster, .)

pipe_pr <- 
  pipe_cl %>% 
  fx_prepare_predict() %>% 
  ml_fit_and_transform(pipeline_predict("fare"), .) %>% 
  select(cluster:pickup_hour, last_col())

```


## Extra Notes

-- use spark pipelines
-- use spark streaming
-- how does this work with rmd / shiny app?

## Disconnect Spark

-- toggle end of the spark app
```{r disconnect}
spark_disconnect(sc)
```

