---
title: "Big Data in R with Spark"
author: "Kevin Putschko"
date: "`r Sys.Date()`"
output: 
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, error = FALSE)

pacman::p_load(tidyverse, leaflet, DT, lubridate,
               colorspace, extrafont, scales, ggthemes,
               patchwork, RColorBrewer)


report_assets <- read_rds("models/2020-02-26/summary.rds")

report_dtmin <- report_assets$s_summary$date_min %>% format("%b %Y")
report_dtmax <- report_assets$s_summary$date_max %>% format("%b %Y")
```

### Our Test Cases

Using `R 3.6.2`, with `Spark 2.4` and `sparklyr 1.1` on the following computing environments: 

Environment | Resources | Analogy
------------|-----------|---------
Experis Laptop | 8GB RAM, 1 CPU, 4 Core; Intel i5 |  Just you, doing research in a library
AWS EC2 Server | 32GB RAM, 8 CPU, 1 Core; t2.2xlarge | You with a cart, doing research in a library
Databricks Spark Cluster | 32GB RAM, 1 Driver, 4 Worker; i3.xlarge  | You, acting as the project manager, overseeing 4 assistants doing research in a library

### New York City Taxi Requests

We see a representation of the taxi requests in early 2009.  The requests seem to be centered around the middle of Manhattan, with fewer requests as you move north, and east into Brooklyn.

```{r requests}
map_center <-
  report_assets$s_centers %>%
  summarise_at(vars(center_lon, center_lat), mean) %>%
  as.list()

report_assets$s_centers %>%
  leaflet(width = "100%") %>%
  addProviderTiles(providers$CartoDB.DarkMatterNoLabels, group = "New York City") %>%
  addCircles(lng = ~ center_lon,
             lat = ~ center_lat,
             label = ~str_c("Hub - ", taxi_hub),
             color = ~hub_color,
             stroke = TRUE,
             weight = 2.5,
             opacity = 0.75,
             fillColor = "transparent",
             radius = ~if_else(hub_pct > .20, 2500, 5000),
             group = "Hub Clusters") %>%
  addCircleMarkers(data = report_assets$s_blackmap,
                   lng = ~lon_r,
                   lat = ~lat_r,
                   fillOpacity = 0.075,
                   radius = ~log(n),
                   stroke = FALSE,
                   color = "yellow",
                   popup = NULL,
                   group = "Taxi Requests") %>%
  addLayersControl(overlayGroups = c("Taxi Requests", "Hub Clusters"),
                   options = layersControlOptions(collapsed = FALSE)) %>%
  hideGroup("Hub Clusters") %>%
  setView(lng = map_center$center_lon, lat = map_center$center_lat, zoom = 10)
```


### New York City Taxi - Heat Map

Notice the lull in the early morning, and then an increase in taxi requests as rush hour approaches.

```{r heatmap-1, fig.align='center', fig.height=3, cache=TRUE}
report_assets$s_heatmap %>%
  ggplot(aes(x = pickup_hour, y = pickup_wday, fill = rides)) +
  geom_tile() +
  coord_equal() +
  scale_fill_viridis_c(label = number_format(big.mark = ",", scale = 1/1000, suffix = "k")) +
  labs(title = "New York City Taxi Service", 
       subtitle = "Total number of taxi rides by hour and weekday",
       fill = NULL,
       x = NULL,
       y = NULL,
       caption = str_glue("Data ranges from {report_dtmin} to {report_dtmax}")) +
  theme_minimal(base_family = "Trebuchet MS") +
  theme(plot.title = element_text(family = "Calibri", face = "bold", size = 16))
```

We can see here that the fare per hour peaks at around $5/mile in the later weekdays, and again at the beginning and end of the working days.  

```{r heatmap-2, fig.align='center', fig.height=3, cache=TRUE}
report_assets$s_heatmap %>%
  ggplot(aes(x = pickup_hour, y = pickup_wday, fill = fpm)) +
  geom_tile() +
  coord_equal() +
  scale_fill_viridis_c(label = dollar_format()) +
  labs(title = "New York City Taxi Service", 
       subtitle = "Average fare per mile by hour and weekday",
       fill = NULL,
       x = NULL,
       y = NULL,
       caption = str_glue("Data ranges from {report_dtmin} to {report_dtmax}")) +
  theme_minimal(base_family = "Trebuchet MS") +
  theme(plot.title = element_text(family = "Calibri", face = "bold", size = 16))
```

### Environment Comparison

My laptop crashed when trying to build models on 40 million rows of data, but it was able to convert the files to Spark Parquet format just fine.  The laptop was able to complete the process when working with just 20 millions rows at 5gb of RAM reserved for Spark.  The same process takes nearly 1/3 the amount of time on the AWS EC2 Server with a similar 5gb of RAM reserved for Spark.  

```{r clean-logs, fig.align='center', fig.height=6, fig.width=8}

raw_logs <- 
  read_csv("plot-runtime-logs.csv", col_types = cols()) %>% 
  mutate(label = str_c(env, ram, sep = "\n")) %>% 
  mutate_at(vars(env, ram, rows, label), fct_inorder) %>% 
  mutate_at(vars(stage), fct_rev) 

log_colors <- 
  c("Write Parquet and Build Pipelines" = "#FC8D62",
    "Build Models" = "#66C2A5")

plot_40 <- 
  raw_logs %>% 
  filter(group == "B") %>% 
  ggplot(aes(x = label, y = min, fill = stage, color = stage)) +
  geom_col(width = .45) +
  scale_y_continuous(labels = label_number(accuracy = 1, suffix = " mins"),
                     breaks = c(10, 20),
                     minor_breaks = 0) +
  scale_color_manual(values = modify(log_colors, darken, amount = 0.20)) +
  scale_fill_manual(values = log_colors) +
  labs(x = NULL, y = NULL, fill = NULL, color = NULL, subtitle = "40 Million Rows") +
  theme_fivethirtyeight(base_family = "Trebuchet MS") +
  theme(plot.subtitle = element_text(hjust = 0.50),
        panel.grid.minor.y = element_line(color = "lightgray"),
        panel.grid.major.x = element_blank())
  
plot_20 <-
  raw_logs %>% 
  filter(group == "A") %>% 
  ggplot(aes(x = label, y = min, fill = stage, color = stage)) +
  geom_col(width = 0.50) +
  scale_y_continuous(labels = label_number(accuracy = 1),
                     breaks = c(20, 40),
                     minor_breaks = 0) +
  scale_color_manual(values = modify(log_colors, darken, amount = 0.20)) +
  scale_fill_manual(values = log_colors) +
  labs(x = NULL, y = NULL, fill = NULL, color = NULL, subtitle = "20 Million Rows") +
  guides(color = "none", fill = "none") +
  theme_fivethirtyeight(base_family = "Trebuchet MS") +
  theme(plot.subtitle = element_text(hjust = 0.50),
        panel.grid.minor.y = element_line(color = "lightgray"),
        panel.grid.major.x = element_blank())

plot_340 <-
  raw_logs %>% 
  filter(group == "C") %>% 
  ggplot(aes(x = label, y = min, fill = stage, color = stage)) +
  geom_col(width = 0.50) +
  scale_y_continuous(labels = label_number(accuracy = 1),
                     breaks = c(20, 100),
                     minor_breaks = pretty_breaks(5)) +
  scale_color_manual(values = modify(log_colors, darken, amount = 0.20)) +
  scale_fill_manual(values = log_colors) +
  labs(x = NULL, y = NULL, fill = NULL, color = NULL, subtitle = "340 Million Rows") +
  guides(fill = "none", color = "none") +
  theme_fivethirtyeight(base_family = "Trebuchet MS") +
  theme(plot.subtitle = element_text(hjust = 0.50),
        panel.grid.minor.y = element_line(color = lighten("darkgray", 0.20)),
        panel.grid.major.x = element_blank())


plot_40 / (plot_20 + plot_340) + 
  plot_annotation(title = "Runtimes on Various Spark Environments", 
                  theme = theme_fivethirtyeight(base_family = "Calibri")) +
  plot_layout(guides = "collect")
```


### References

1. The R in Spark: https://therinspark.com/
2. Sparklyr Documentation: https://spark.rstudio.com/
3. Databricks: https://databricks.com/try-databricks

